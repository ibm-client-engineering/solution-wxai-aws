{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdfplumber in /Users/shirleyhan/miniconda3/envs/genai/lib/python3.11/site-packages (0.10.3)\n",
      "Requirement already satisfied: pdfminer.six==20221105 in /Users/shirleyhan/miniconda3/envs/genai/lib/python3.11/site-packages (from pdfplumber) (20221105)\n",
      "Requirement already satisfied: Pillow>=9.1 in /Users/shirleyhan/miniconda3/envs/genai/lib/python3.11/site-packages (from pdfplumber) (10.1.0)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in /Users/shirleyhan/miniconda3/envs/genai/lib/python3.11/site-packages (from pdfplumber) (4.23.1)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in /Users/shirleyhan/miniconda3/envs/genai/lib/python3.11/site-packages (from pdfminer.six==20221105->pdfplumber) (2.0.4)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /Users/shirleyhan/miniconda3/envs/genai/lib/python3.11/site-packages (from pdfminer.six==20221105->pdfplumber) (39.0.1)\n",
      "Requirement already satisfied: cffi>=1.12 in /Users/shirleyhan/miniconda3/envs/genai/lib/python3.11/site-packages (from cryptography>=36.0.0->pdfminer.six==20221105->pdfplumber) (1.15.1)\n",
      "Requirement already satisfied: pycparser in /Users/shirleyhan/miniconda3/envs/genai/lib/python3.11/site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20221105->pdfplumber) (2.21)\n",
      "Requirement already satisfied: ibm-watson-machine-learning in /Users/shirleyhan/.local/lib/python3.11/site-packages (1.0.355)\n",
      "Requirement already satisfied: requests in /Users/shirleyhan/miniconda3/envs/genai/lib/python3.11/site-packages (from ibm-watson-machine-learning) (2.31.0)\n",
      "Requirement already satisfied: urllib3 in /Users/shirleyhan/miniconda3/envs/genai/lib/python3.11/site-packages (from ibm-watson-machine-learning) (1.26.16)\n",
      "Requirement already satisfied: pandas<2.2.0,>=0.24.2 in /Users/shirleyhan/miniconda3/envs/genai/lib/python3.11/site-packages (from ibm-watson-machine-learning) (2.1.1)\n",
      "Requirement already satisfied: certifi in /Users/shirleyhan/miniconda3/envs/genai/lib/python3.11/site-packages (from ibm-watson-machine-learning) (2023.7.22)\n",
      "Requirement already satisfied: lomond in /Users/shirleyhan/miniconda3/envs/genai/lib/python3.11/site-packages (from ibm-watson-machine-learning) (0.3.3)\n",
      "Requirement already satisfied: tabulate in /Users/shirleyhan/miniconda3/envs/genai/lib/python3.11/site-packages (from ibm-watson-machine-learning) (0.9.0)\n",
      "Requirement already satisfied: packaging in /Users/shirleyhan/miniconda3/envs/genai/lib/python3.11/site-packages (from ibm-watson-machine-learning) (23.0)\n",
      "Requirement already satisfied: ibm-cos-sdk<2.14.0,>=2.12.0 in /Users/shirleyhan/miniconda3/envs/genai/lib/python3.11/site-packages (from ibm-watson-machine-learning) (2.13.1)\n",
      "Requirement already satisfied: importlib-metadata in /Users/shirleyhan/miniconda3/envs/genai/lib/python3.11/site-packages (from ibm-watson-machine-learning) (6.8.0)\n",
      "Requirement already satisfied: ibm-cos-sdk-core==2.13.1 in /Users/shirleyhan/miniconda3/envs/genai/lib/python3.11/site-packages (from ibm-cos-sdk<2.14.0,>=2.12.0->ibm-watson-machine-learning) (2.13.1)\n",
      "Requirement already satisfied: ibm-cos-sdk-s3transfer==2.13.1 in /Users/shirleyhan/miniconda3/envs/genai/lib/python3.11/site-packages (from ibm-cos-sdk<2.14.0,>=2.12.0->ibm-watson-machine-learning) (2.13.1)\n",
      "Requirement already satisfied: jmespath<=1.0.1,>=0.10.0 in /Users/shirleyhan/miniconda3/envs/genai/lib/python3.11/site-packages (from ibm-cos-sdk<2.14.0,>=2.12.0->ibm-watson-machine-learning) (1.0.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.2 in /Users/shirleyhan/miniconda3/envs/genai/lib/python3.11/site-packages (from ibm-cos-sdk-core==2.13.1->ibm-cos-sdk<2.14.0,>=2.12.0->ibm-watson-machine-learning) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /Users/shirleyhan/miniconda3/envs/genai/lib/python3.11/site-packages (from pandas<2.2.0,>=0.24.2->ibm-watson-machine-learning) (1.25.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/shirleyhan/miniconda3/envs/genai/lib/python3.11/site-packages (from pandas<2.2.0,>=0.24.2->ibm-watson-machine-learning) (2022.7)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/shirleyhan/miniconda3/envs/genai/lib/python3.11/site-packages (from pandas<2.2.0,>=0.24.2->ibm-watson-machine-learning) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/shirleyhan/miniconda3/envs/genai/lib/python3.11/site-packages (from requests->ibm-watson-machine-learning) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/shirleyhan/miniconda3/envs/genai/lib/python3.11/site-packages (from requests->ibm-watson-machine-learning) (3.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/shirleyhan/miniconda3/envs/genai/lib/python3.11/site-packages (from importlib-metadata->ibm-watson-machine-learning) (3.16.1)\n",
      "Requirement already satisfied: six>=1.10.0 in /Users/shirleyhan/miniconda3/envs/genai/lib/python3.11/site-packages (from lomond->ibm-watson-machine-learning) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install pdfplumber\n",
    "! pip install ibm-watson-machine-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "from pdfplumber.utils import cluster_objects\n",
    "from operator import itemgetter\n",
    "from ibm_watson_machine_learning import APIClient\n",
    "from ibm_watsonx_ai.foundation_models.utils.enums import ModelTypes\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watsonx_ai.foundation_models import ModelInference\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credentials\n",
    "SPACE_ID = os.getenv(\"wml_space_id\")\n",
    "IBM_CLOUD_APIKEY = os.getenv(\"ibm_cloud_apikey\")\n",
    "WX_PROJECT_ID = os.getenv(\"wx_project_id\")\n",
    "CREDS = {\n",
    "    \"url\": \"https://us-south.ml.cloud.ibm.com\",\n",
    "    \"apikey\": IBM_CLOUD_APIKEY\n",
    "}\n",
    "project_id = WX_PROJECT_ID\n",
    "\n",
    "# Parameters and prompts used\n",
    "model_type = ModelTypes.MIXTRAL_8X7B_INSTRUCT_V01_Q\n",
    "decoding_method = \"greedy\"\n",
    "max_tokens = 500\n",
    "min_tokens = 50\n",
    "repetition_penalty = 1.05\n",
    "instruction = \"\"\"\n",
    "Read and understand the table, then transform the given table into a coherent paragraph. The first row is the column heading. Please keep it as similar as to the original text. Do not include any explanation in the output. Make sure to include every word from the given string. Do not use bullet points.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_file_paths(directory):\n",
    "    \"\"\"\n",
    "    Get all file paths from different directories\n",
    "    \"\"\"\n",
    "    file_paths = [] \n",
    "\n",
    "    # Check all directories\n",
    "    for root, directories, files in os.walk(directory):\n",
    "        for filename in files:\n",
    "            # Create the full filepath by joining the root directory with the filename\n",
    "            filepath = os.path.join(root, filename)\n",
    "            file_paths.append(filepath)\n",
    "\n",
    "    return file_paths\n",
    "\n",
    "def check_bboxes(word, table_bbox):\n",
    "    \"\"\"\n",
    "    Check whether word is inside a table bbox.\n",
    "    \"\"\"\n",
    "    l = word['x0'], word['top'], word['x1'], word['bottom']\n",
    "    r = table_bbox\n",
    "    return l[0] > r[0] and l[1] > r[1] and l[2] < r[2] and l[3] < r[3]\n",
    "\n",
    "def clean_and_combine_headers(rows):\n",
    "    \"\"\"Clean tables and refine table column headers\"\"\"\n",
    "    if not rows:\n",
    "        return rows\n",
    "\n",
    "    # Take the first row as the header\n",
    "    header_row = rows[0]\n",
    "\n",
    "    # Create a new table starting from the first row\n",
    "    new_table = [header_row] + rows[1:]\n",
    "\n",
    "    # Transpose back to columns to filter out columns with all None values\n",
    "    transposed_rows = list(zip(*new_table))\n",
    "    final_columns = []\n",
    "    for col in transposed_rows:\n",
    "        if any(cell is not None and cell.strip() for cell in col):\n",
    "            final_columns.append(col)\n",
    "\n",
    "    # Transpose back to rows\n",
    "    final_rows = list(zip(*final_columns))\n",
    "\n",
    "    return final_rows\n",
    "\n",
    "def clean_sme_text(table, filename):\n",
    "    \"\"\"Replace 'Subject Matter Experts:' with 'For {filename}, the subject matter experts are:' in table rows.\"\"\"\n",
    "    for i, row in enumerate(table):\n",
    "        table[i] = [cell.replace(\"Subject Matter Experts:\", f\"For {filename}, the subject matter experts are:\") if cell and \"Subject Matter Experts:\" in cell else cell for cell in row]\n",
    "    return table\n",
    "\n",
    "def remove_bullet_points(lines):\n",
    "    \"\"\"Remove main bullets. Convert sub-bullets ending with ' o' into comma-separated text enclosed by parentheses behind their respective parent bullet.\"\"\"\n",
    "    processed_lines = []\n",
    "    sub_bullets = []\n",
    "\n",
    "    for line in lines:\n",
    "        if isinstance(line, list):  # This means it is a table\n",
    "            if sub_bullets and processed_lines:\n",
    "                processed_lines[-1] += ' (' + ', '.join(sub_bullets) + ')'\n",
    "                sub_bullets = []\n",
    "            processed_lines.append(line)\n",
    "        else:  # This means it is text\n",
    "            line = line.replace(\"â€¢\", \"\") #remove main bullet\n",
    "            stripped_line = line.strip()\n",
    "            # Logic for processing sub bullets\n",
    "            if stripped_line.endswith(' o'): \n",
    "                # remove sub-bullets\n",
    "                sub_bullet_text = stripped_line[:-2].strip()\n",
    "                if sub_bullet_text:\n",
    "                    sub_bullets.append(sub_bullet_text)\n",
    "            else:\n",
    "                # Combine sub bullets\n",
    "                if sub_bullets and processed_lines:\n",
    "                    processed_lines[-1] += ' (' + ', '.join(sub_bullets) + ')'\n",
    "                    sub_bullets = []\n",
    "                processed_lines.append(line)\n",
    "\n",
    "    # In case there are sub-bullets left unprocessed\n",
    "    if sub_bullets and processed_lines and not isinstance(processed_lines[-1], list):\n",
    "        processed_lines[-1] += ' (' + ', '.join(sub_bullets) + ')'\n",
    "\n",
    "    return processed_lines\n",
    "\n",
    "def watsonx_ai_api(creds, model_id, decoding_method, max_tokens, min_tokens, repetition_penalty, instruction, context):\n",
    "    \"\"\"Calling the watsonx.ai API\"\"\"\n",
    "    parameters = {\n",
    "        GenParams.DECODING_METHOD: decoding_method,\n",
    "        GenParams.MAX_NEW_TOKENS: max_tokens,\n",
    "        GenParams.MIN_NEW_TOKENS: min_tokens,\n",
    "        GenParams.REPETITION_PENALTY: repetition_penalty,\n",
    "        GenParams.STOP_SEQUENCES: [\"\\n\\n\"]\n",
    "    }\n",
    "    model = ModelInference(\n",
    "        model_id=model_id, \n",
    "        params=parameters, \n",
    "        credentials=creds,\n",
    "        project_id=project_id)\n",
    "\n",
    "    result = model.generate_text(\" \".join([instruction, context]))\n",
    "    return result\n",
    "\n",
    "def process_file(pdf_path, html_path):\n",
    "    \"\"\"Main function to preprocess a PDF file and output a HTML file\"\"\"\n",
    "    footer_threshold = 50  # footer region\n",
    "    header_threshold = 50  # header region\n",
    "    filename = os.path.splitext(os.path.basename(pdf_path))[0]  # Extract the filename without extension\n",
    "\n",
    "    # Process the PDF\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        all_html = \"<html><body>\"\n",
    "\n",
    "        # Process page by page\n",
    "        for page_number, page in enumerate(pdf.pages, start=1):\n",
    "            # Find tables and get the bounding boxes and get table data\n",
    "            tables = page.find_tables()\n",
    "            table_bboxes = [i.bbox for i in tables]\n",
    "            tables_data = [{'table': i.extract(), 'top': i.bbox[1]} for i in tables]\n",
    "\n",
    "            # Extract non-table text and exclude footer text and header text \n",
    "            page_height = page.height\n",
    "            non_table_words = [\n",
    "                word for word in page.extract_words()\n",
    "                if not any([check_bboxes(word, table_bbox) for table_bbox in table_bboxes])\n",
    "                and word['bottom'] < page_height - footer_threshold\n",
    "                 # Starting from second page and onwards because header only exists starting page 2\n",
    "                and (page_number == 1 or word['top'] > header_threshold)\n",
    "            ]\n",
    "            \n",
    "            #Ensure we append table at the right location, so we can replace table with summarized text later\n",
    "            lines = []\n",
    "            for cluster in cluster_objects(non_table_words + tables_data, itemgetter('top'), tolerance=5):\n",
    "                if isinstance(cluster[0], dict) and 'text' in cluster[0]:  # Check if the first element is a word\n",
    "                    lines.append(' '.join([i['text'] for i in cluster if 'text' in i]))\n",
    "                elif 'table' in cluster[0]:\n",
    "                    lines.append(cluster[0]['table'])\n",
    "\n",
    "            # Group and format main bullet and sub-bullets\n",
    "            processed_lines = remove_bullet_points(lines)\n",
    "\n",
    "            # Add text and tables to HTML\n",
    "            for line in processed_lines:\n",
    "                if isinstance(line, list):  # This is a table\n",
    "                    table = clean_and_combine_headers(line)\n",
    "                    table = clean_sme_text(table, filename)  # Replace SME text in the table\n",
    "                    if len(table) > 1 and len(table[0]) > 0:  # Ensure the table has the correct structure\n",
    "                        df = pd.DataFrame(table[1:], columns=table[0])\n",
    "                        table_string = \"Input: \" + df.to_string(index=False) +\"Output: \"\n",
    "                        # Pass the table information to watsonx.ai API to have it summarized.\n",
    "                        result = watsonx_ai_api(\n",
    "                            CREDS, model_type, decoding_method, max_tokens, min_tokens,\n",
    "                            repetition_penalty, instruction, table_string\n",
    "                        )\n",
    "                        all_html += f\"<p>{result}</p>\"\n",
    "                else:  # This is text\n",
    "                    all_html += f\"<p>{line}</p>\"\n",
    "\n",
    "        all_html += \"</body></html>\"\n",
    "\n",
    "    #write the html file\n",
    "    with open(html_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(all_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Folder paths\n",
    "directory = [\n",
    "    \"Filtered Pdf's/Complexity- High/Business/\", \"Filtered Pdf's/Complexity- High/Technical/\",\n",
    "    \"Filtered Pdf's/Complexity- Medium/Business/\", \"Filtered Pdf's/Complexity- Medium/Technical/\",\n",
    "    \"Filtered Pdf's/Complexity- Easy/Business/\", \"Filtered Pdf's/Complexity- Easy/Technical/\",\n",
    "]\n",
    "\n",
    "#Find all file paths\n",
    "final_path = []\n",
    "for dir in directory:\n",
    "    file_paths = get_all_file_paths(dir)\n",
    "    for path in file_paths:\n",
    "        final_path.append(path)\n",
    "\n",
    "# If output directory does not exist, create html folder\n",
    "output_dir = 'html2'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Run through each file \n",
    "for pdf_path in final_path:\n",
    "    filename = os.path.splitext(os.path.basename(pdf_path))[0]\n",
    "    if pdf_path.split('.')[-1] == 'pdf':\n",
    "        html_path = os.path.join(output_dir, f\"{filename}.html\")\n",
    "        process_file(pdf_path, html_path)\n",
    "        print(f\"{filename}.html is created.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
